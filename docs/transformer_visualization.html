<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer Architecture Visualization</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background-color: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #333;
            text-align: center;
        }
        .image-container {
            display: flex;
            justify-content: center;
            margin: 20px 0;
        }
        img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        .description {
            margin-top: 20px;
            line-height: 1.6;
        }
        footer {
            margin-top: 40px;
            text-align: center;
            color: #666;
            font-size: 0.9em;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Transformer Model Architecture</h1>
        <p>The following diagram illustrates the architecture of the Transformer model as implemented in this project.</p>
        
        <div class="image-container">
            <img src="transformer.webp" alt="Transformer Model Architecture Diagram showing Encoder and Decoder layers with Multi-Head Attention, Feed Forward networks, Add &amp; Norm blocks, Positional Encoding, Input Embedding, Output Embedding, Softmax, Linear layer, and Output Probabilities">
        </div>
        
        <div class="description">
            <h2>Architecture Overview</h2>
            <p>The Transformer model consists of an encoder stack and a decoder stack. The encoder processes the input sequence through multiple identical layers, each containing:</p>
            <ul>
                <li><strong>Multi-Head Attention</strong>: Allows the model to jointly attend to information from different representation subspaces at different positions.</li>
                <li><strong>Feed Forward Network</strong>: A position-wise fully connected feed-forward network applied to each position separately and identically.</li>
                <li><strong>Add &amp; Norm</strong>: Residual connections followed by layer normalization for improved training stability.</li>
            </ul>
            
            <p>The decoder generates the output sequence one element at a time, with each element attending to the previously generated elements. Each decoder layer contains:</p>
            <ul>
                <li><strong>Masked Multi-Head Attention</strong>: Prevents the model from attending to future positions in the target sequence during training.</li>
                <li><strong>Multi-Head Attention</strong>: Attends to the encoder's output.</li>
                <li><strong>Feed Forward Network</strong>: Same as in the encoder.</li>
                <li><strong>Add &amp; Norm</strong>: Residual connections followed by layer normalization.</li>
            </ul>
            
            <p>The model uses positional encoding to add information about the position of tokens in the sequence since the self-attention mechanism has no inherent notion of order.</p>
        </div>
        
        <footer>
            <p>Visualization created based on the original "Attention Is All You Need" paper by Vaswani et al.</p>
            <p>This implementation follows the architecture described in the paper with modifications for educational clarity.</p>
        </footer>
    </div>
</body>
</html>