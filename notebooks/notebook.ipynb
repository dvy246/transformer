{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba09372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd \n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "548d4c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Aarif1430/english-to-hindi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5b2f91fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.DataFrame(ds[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "de76fc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"However, Paes, who was partnering Australia's Paul Hanley, could only go as far as the quarterfinals where they lost to Bhupathi and Knowles\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2ac6741c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाल...\n",
       "1         और जो शख्स (अपने आमाल का) बदला दुनिया ही में च...\n",
       "2         जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि ...\n",
       "3           आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को\n",
       "4         8 सितम्‍बर, 2016 को माननीय राष्‍ट्रपति की स्‍व...\n",
       "                                ...                        \n",
       "127700    आर्ट डेको शैली के निर्माण मैरीन ड्राइव और ओवल ...\n",
       "127701                      और अपने गालों में डाल लेते हैं।\n",
       "127702    जहां तक गंधक के अन्य उत्पादों का प्रश्न है , द...\n",
       "127703    Zरचना-प्रकिया को उसने एक पहेली में यों बांधा है .\n",
       "127704    हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ ...\n",
       "Name: hindi_sentence, Length: 127705, dtype: str"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['hindi_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dfa86b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>127705</td>\n",
       "      <td>127705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>124417</td>\n",
       "      <td>97762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>(Laughter)</td>\n",
       "      <td>(हँसी)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>555</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       english_sentence hindi_sentence\n",
       "count            127705         127705\n",
       "unique           124417          97762\n",
       "top          (Laughter)         (हँसी)\n",
       "freq                555            212"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e10210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cadd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Logits: tensor([[-0.0721, -0.0458]], grad_fn=<AddmmBackward0>)\n",
      "Probabilities: tensor([[0.4934, 0.5066]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 1. Load the model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# We tell the model to expect 2 categories (e.g., 'Positive', 'Negative')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# 2. Prepare an input\n",
    "text = \"This tutorial is incredibly detailed!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 3. Run the model\n",
    "# Internally: \n",
    "#   - Inputs go into the DistilBERT Body.\n",
    "#   - Body outputs a vector (hidden state) of size 768.\n",
    "#   - The 'Head' takes that 768 vector and projects it to 2 numbers (logits).\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# 4. Access the Logits\n",
    "# Logits are raw scores from the final layer BEFORE the softmax.\n",
    "# They are NOT probabilities yet (they can be negative or > 1).\n",
    "logits = outputs.logits\n",
    "\n",
    "# 5. Convert Logits to Probabilities\n",
    "# torch.nn.functional.softmax turns scores into 0-1 range that sums to 1.\n",
    "predictions = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "print(f\"Raw Logits: {logits}\")\n",
    "print(f\"Probabilities: {predictions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cc2aa2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/transformer/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4025\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.batch_decode\u001b[39m\u001b[34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   4000\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbatch_decode\u001b[39m(\n\u001b[32m   4001\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4002\u001b[39m     sequences: Union[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]], \u001b[33m\"\u001b[39m\u001b[33mnp.ndarray\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtorch.Tensor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtf.Tensor\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   4005\u001b[39m     **kwargs,\n\u001b[32m   4006\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m   4007\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4008\u001b[39m \u001b[33;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[32m   4009\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4022\u001b[39m \u001b[33;03m        `list[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[32m   4023\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m-> \u001b[39m\u001b[32m4025\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4026\u001b[39m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4027\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4028\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4029\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4030\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4031\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[32m   4032\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/transformer/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4064\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   4061\u001b[39m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[32m   4062\u001b[39m token_ids = to_py_obj(token_ids)\n\u001b[32m-> \u001b[39m\u001b[32m4064\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4068\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4069\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/transformer/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:682\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    681\u001b[39m     token_ids = [token_ids]\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    684\u001b[39m clean_up_tokenization_spaces = (\n\u001b[32m    685\u001b[39m     clean_up_tokenization_spaces\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    687\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.clean_up_tokenization_spaces\n\u001b[32m    688\u001b[39m )\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[31mTypeError\u001b[39m: argument 'ids': 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "tokenizer.batch_decode(logits, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a2388a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: The secret to learning AI is to build it from the ground up. That means that, even if you can make an AI that does something you'd never dream of doing, you\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 1. Load a generative model (decoder-only)\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 2. Tokenize the prompt\n",
    "prompt = \"The secret to learning AI is\"\n",
    "# .to(\"cpu\") or .to(\"cuda\") ensures the data is where the model is.\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# 3. The .generate() method - The engine of creation\n",
    "# - max_new_tokens: How many words to add (ignores prompt length).\n",
    "# - do_sample=True: Enables creative randomness (not just the top word).\n",
    "# - temperature=0.7: 'Cooler' (low) is more factual; 'Hotter' (high) is more random.\n",
    "# - top_p=0.9: 'Nucleus sampling' - only looks at the top 90% of likely words.\n",
    "output_tokens = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=30, \n",
    "    do_sample=True, \n",
    "    temperature=0.7, \n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# 4. Decode the numbers back to words\n",
    "# batch_decode handles a list of sequences; [0] takes the first (and only) result.\n",
    "# skip_special_tokens=True removes technical markers like <|endoftext|>.\n",
    "decoded_text = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "print(f\"Generated: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "928948f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e41bcf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor=torch.tensor(data=[[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30371be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8a8fd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error importing huggingface_hub.hf_file_system: cannot import name 'http_stream_backoff' from 'huggingface_hub.utils' (/Users/divyyadav/Desktop/transformer/.venv/lib/python3.12/site-packages/huggingface_hub/utils/__init__.py)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'http_stream_backoff' from 'huggingface_hub.utils' (/Users/divyyadav/Desktop/transformer/.venv/lib/python3.12/site-packages/huggingface_hub/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset \u001b[38;5;66;03m# A sister library for efficient data loading\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1. THE DATA (The \"Fuel\")\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# We load a simple dataset. Internally, this is handled as a memory-mapped \u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# file so it doesn't crash your RAM even if the dataset is 100GB.\u001b[39;00m\n\u001b[32m      7\u001b[39m dataset = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mimdb\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/transformer/.venv/lib/python3.12/site-packages/datasets/__init__.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m4.5.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Column, Dataset\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/transformer/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:77\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontrib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconcurrent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_writer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_files\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sanitize_patterns\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/transformer/.venv/lib/python3.12/site-packages/datasets/arrow_reader.py:30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpq\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontrib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconcurrent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownload\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownload_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnaming\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _split_re, filenames_for_dataset_split\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtable\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InMemoryTable, MemoryMappedTable, Table, concat_tables\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/transformer/.venv/lib/python3.12/site-packages/datasets/download/__init__.py:9\u001b[39m\n\u001b[32m      1\u001b[39m __all__ = [\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDownloadConfig\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDownloadManager\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDownloadMode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mStreamingDownloadManager\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m ]\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownload_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownload_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DownloadManager, DownloadMode\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstreaming_download_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StreamingDownloadManager\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/transformer/.venv/lib/python3.12/site-packages/datasets/download/download_manager.py:31\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontrib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconcurrent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm \u001b[38;5;28;01mas\u001b[39;00m hf_tqdm\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfile_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     33\u001b[39m     ArchiveIterable,\n\u001b[32m     34\u001b[39m     FilesIterable,\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m     url_or_path_join,\n\u001b[32m     39\u001b[39m )\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minfo_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_size_checksum_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/transformer/.venv/lib/python3.12/site-packages/datasets/utils/__init__.py:17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm \u001b[38;5;28;01mas\u001b[39;00m _tqdm  \u001b[38;5;66;03m# _tqdm is the module\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01minfo_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VerificationMode\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m disable_progress_bar, enable_progress_bar, is_progress_bar_enabled\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     are_progress_bars_disabled,\n\u001b[32m     21\u001b[39m     disable_progress_bars,\n\u001b[32m     22\u001b[39m     enable_progress_bars,\n\u001b[32m     23\u001b[39m     tqdm,\n\u001b[32m     24\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/transformer/.venv/lib/python3.12/site-packages/datasets/utils/info_utils.py:8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m insecure_hashlib\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     ExpectedMoreDownloadedFilesError,\n\u001b[32m     10\u001b[39m     ExpectedMoreSplitsError,\n\u001b[32m     11\u001b[39m     NonMatchingChecksumError,\n\u001b[32m     12\u001b[39m     NonMatchingSplitsSizesError,\n\u001b[32m     13\u001b[39m     UnexpectedDownloadedFileError,\n\u001b[32m     14\u001b[39m     UnexpectedSplitsError,\n\u001b[32m     15\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_logger\n\u001b[32m     19\u001b[39m logger = get_logger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/transformer/.venv/lib/python3.12/site-packages/datasets/exceptions.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# SPDX-License-Identifier: Apache-2.0\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Copyright 2023 The HuggingFace Authors.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Optional, Union\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HfFileSystem\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtable\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CastError\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/transformer/.venv/lib/python3.12/site-packages/huggingface_hub/__init__.py:1044\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_attach\u001b[39m(package_name, submodules=\u001b[38;5;28;01mNone\u001b[39;00m, submod_attrs=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1019\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Attach lazily loaded submodules, functions, or other attributes.\u001b[39;00m\n\u001b[32m   1020\u001b[39m \n\u001b[32m   1021\u001b[39m \u001b[33;03m    Typically, modules import submodules and attributes as follows:\u001b[39;00m\n\u001b[32m   1022\u001b[39m \n\u001b[32m   1023\u001b[39m \u001b[33;03m    ```py\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[33;03m    import mysubmodule\u001b[39;00m\n\u001b[32m   1025\u001b[39m \u001b[33;03m    import anothersubmodule\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03m    from .foo import someattr\u001b[39;00m\n\u001b[32m   1028\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m   1029\u001b[39m \n\u001b[32m   1030\u001b[39m \u001b[33;03m    The idea is to replace a package's `__getattr__`, `__dir__`, such that all imports\u001b[39;00m\n\u001b[32m   1031\u001b[39m \u001b[33;03m    work exactly the way they would with normal imports, except that the import occurs\u001b[39;00m\n\u001b[32m   1032\u001b[39m \u001b[33;03m    upon first use.\u001b[39;00m\n\u001b[32m   1033\u001b[39m \n\u001b[32m   1034\u001b[39m \u001b[33;03m    The typical way to call this function, replacing the above imports, is:\u001b[39;00m\n\u001b[32m   1035\u001b[39m \n\u001b[32m   1036\u001b[39m \u001b[33;03m    ```python\u001b[39;00m\n\u001b[32m   1037\u001b[39m \u001b[33;03m    __getattr__, __dir__ = lazy.attach(\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[33;03m        __name__,\u001b[39;00m\n\u001b[32m   1039\u001b[39m \u001b[33;03m        ['mysubmodule', 'anothersubmodule'],\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[33;03m        {'foo': ['someattr']}\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[33;03m    )\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m   1043\u001b[39m \u001b[33;03m    This functionality requires Python 3.7 or higher.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m \n\u001b[32m   1045\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   1046\u001b[39m \u001b[33;03m        package_name (`str`):\u001b[39;00m\n\u001b[32m   1047\u001b[39m \u001b[33;03m            Typically use `__name__`.\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[33;03m        submodules (`set`):\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[33;03m            List of submodules to attach.\u001b[39;00m\n\u001b[32m   1050\u001b[39m \u001b[33;03m        submod_attrs (`dict`):\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[33;03m            Dictionary of submodule -> list of attributes / functions.\u001b[39;00m\n\u001b[32m   1052\u001b[39m \u001b[33;03m            These attributes are imported as they are used.\u001b[39;00m\n\u001b[32m   1053\u001b[39m \n\u001b[32m   1054\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[33;03m        __getattr__, __dir__, __all__\u001b[39;00m\n\u001b[32m   1056\u001b[39m \n\u001b[32m   1057\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1058\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m submod_attrs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1059\u001b[39m         submod_attrs = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/transformer/.venv/lib/python3.12/site-packages/huggingface_hub/hf_file_system.py:25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfile_download\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hf_hub_url, http_get\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhf_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HfApi, LastCommitInfo, RepoFile\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HFValidationError, hf_raise_for_status, http_backoff, http_stream_backoff\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minsecure_hashlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m md5\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Regex used to match special revisions with \"/\" in them (see #1710)\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'http_stream_backoff' from 'huggingface_hub.utils' (/Users/divyyadav/Desktop/transformer/.venv/lib/python3.12/site-packages/huggingface_hub/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset # A sister library for efficient data loading\n",
    "\n",
    "# 1. THE DATA (The \"Fuel\")\n",
    "# We load a simple dataset. Internally, this is handled as a memory-mapped \n",
    "# file so it doesn't crash your RAM even if the dataset is 100GB.\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# 2. THE TOKENIZER (The \"Translator\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Line-by-line function to process the data:\n",
    "def tokenize_function(examples):\n",
    "    # This adds 'input_ids' and 'attention_mask' to the dataset.\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 3. THE MODEL (The \"Student\")\n",
    "# We load the body (DistilBERT) and add a fresh, random \"Head\" for 2-class classification.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# 4. THE ARGUMENTS (The \"Lesson Plan\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    evaluation_strategy=\"epoch\",  # Run a test after every pass through the data\n",
    "    learning_rate=2e-5,           # How big of a 'step' the model takes when learning\n",
    "    weight_decay=0.01,            # A penalty to prevent the model from 'over-memorizing'\n",
    "    num_train_epochs=3,           # Total times to read the whole book\n",
    ")\n",
    "\n",
    "# 5. THE TRAINER (The \"Teacher\")\n",
    "# This object coordinates the Model, the Args, and the Data.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "\n",
    "# 6. THE EXECUTION\n",
    "# This one line replaces ~100 lines of raw PyTorch training loop code.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f689bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=[1,2,3,4,5]\n",
    "\n",
    "b=0\n",
    "\n",
    "for i in arr:\n",
    "    b+=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d5c231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SortMedian:\n",
    "    def __init__(self, arr, arr2):\n",
    "        self.arr = arr\n",
    "        self.arr2 = arr2\n",
    "        self.final_array = self.arr + self.arr2\n",
    "\n",
    "    def combined_median(self, sort: bool = True):\n",
    "        try:\n",
    "            if sort:\n",
    "                sorted_array = sorted(self.final_array)\n",
    "            else:\n",
    "                sorted_array = self.final_array\n",
    "\n",
    "            length = len(sorted_array)\n",
    "\n",
    "            if length % 2 == 0:\n",
    "                x = sorted_array[length//2]\n",
    "                y = sorted_array[length//2 - 1]\n",
    "                return (x + y) / 2\n",
    "            else:\n",
    "                return sorted_array[length//2]\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1945617",
   "metadata": {},
   "outputs": [],
   "source": [
    "medi=SortMedian([1,2,3,4,5], [6,7,8,9,10,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e095d174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medi.final_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8a0c9751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medi.combined_median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d309403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa178920",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=load_dataset(\"Aarif1430/english-to-hindi\",split='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55854b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds,lang):\n",
    "    for items in ds:\n",
    "        yield items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b6a77ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(config,ds,lang):\n",
    "    #define the tokenizer path\n",
    "    path=Path(config[\"tokenizer_path\"].format(lang))\n",
    "\n",
    "    if not path.exists():\n",
    "        #build the tokenizer\n",
    "        tokenizer=Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "        #Pre tokenizer\n",
    "        tokenizer.pre_tokenizer=Whitespace()\n",
    "        #Trainer\n",
    "        trainer=WordLevelTrainer(min_frequency=2,special_tokens=['[PAD]','[UNK]','[SOS]','[EOS]'])\n",
    "\n",
    "        #train the tokenizer using the trainer\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds),trainer=trainer)\n",
    "\n",
    "        #save the tokenizer\n",
    "        tokenizer.save(path)\n",
    "\n",
    "    else:\n",
    "        tokenizer=Tokenizer.from_file(path)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c40e8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
