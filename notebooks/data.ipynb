{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "id": "3dd856f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/divyyadav/transformer/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "3dd856f7",
   "metadata": {},
   "outputs": [],
>>>>>>> 837e97f1282eeabab6674f956e8c28d0cfdc63b5
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset,random_split\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 5,
>>>>>>> 837e97f1282eeabab6674f956e8c28d0cfdc63b5
   "id": "811b9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=load_dataset(\"Aarif1430/english-to-hindi\",split='train')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
   "id": "c2664f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds,lang):\n",
    "          for item in ds:\n",
    "               yield item[lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c805b8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['english_sentence', 'hindi_sentence'],\n",
       "    num_rows: 127705\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
=======
>>>>>>> 837e97f1282eeabab6674f956e8c28d0cfdc63b5
   "execution_count": null,
   "id": "04a02a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds,lang):\n",
    "    for items in ds:\n",
    "        yield items\n",
    "\n",
    "\n",
    "def get_or_build_tokenizer(config,ds,lang):\n",
    "    #define the tokenizer path\n",
    "    path=Path(config[\"tokenizer_path\"].format(lang))\n",
    "\n",
    "    if not path.exists():\n",
    "        #build the tokenizer\n",
    "        tokenizer=Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "        #Pre tokenizer\n",
    "        tokenizer.pre_tokenizer=Whitespace()\n",
    "        #Trainer\n",
    "        trainer=WordLevelTrainer(min_frequency=2,special_tokens=['[PAD]','[UNK]','[SOS]','[EOS]'])\n",
    "\n",
    "        #train the tokenizer using the trainer\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds),trainer=trainer)\n",
    "\n",
    "        #save the tokenizer\n",
    "        tokenizer.save(path)\n",
    "\n",
    "    else:\n",
    "        tokenizer=Tokenizer.from_file(path)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def get_ds(config):\n",
    "\n",
    "    ds=load_dataset(\"Aarif1430/english-to-hindi\",split='train')\n",
    "    \n",
    "    #build tokenizer\n",
    "    train_tokenizer=get_or_build_tokenizer(config=config,ds=ds,lang=\"english_sentence\")\n",
    "    val_tokenizer=get_or_build_tokenizer(config=config,ds=ds,lang=\"hindi_sentence\")\n",
    "\n",
    "    # train test split\n",
    "    train_ds_size=int(0.9*len(ds))\n",
    "\n",
    "    test_ds_size=int(0.1* len(ds))\n",
    "    \n",
    "\n",
    "    #split the data \n",
    "    train_ds_raw,train_ds_val=random_split(ds,[train_ds_size,test_ds_size])\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258ede2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
